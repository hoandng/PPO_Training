import sys
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    BitsAndBytesConfig,
    pipeline,
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    GenerationConfig
)
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training

# --- SETUP PATH ---
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

try:
    from config import Config
except ImportError:
    sys.path.append("/kaggle/working/HistoryGPT-Kaggle-PPO")
    from config import Config


def train_ppo():
    print(">>> [3/4] Train PPO Strategy (Stable + Defensive Patching)...")

    # 1. SETUP GPU
    if torch.cuda.device_count() < 2:
        print("‚ö†Ô∏è Warning: Single GPU detected.")
        device_policy = 0
        device_reward = 0
    else:
        print("‚úÖ Dual GPU detected.")
        device_policy = 0
        device_reward = 1

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )

    # =================================================================
    # 2. LOAD POLICY MODEL -> GPU 0
    # =================================================================
    print(f"Loading Policy Model on cuda:{device_policy}...")

    # A. Load Base Model
    base_model = AutoModelForCausalLM.from_pretrained(
        Config.BASE_MODEL_NAME,
        quantization_config=bnb_config,
        device_map={"": device_policy},
        trust_remote_code=True
    )
    base_model = prepare_model_for_kbit_training(base_model)

    # B. Attach LoRA
    peft_config = LoraConfig(
        r=Config.LORA_R,
        lora_alpha=Config.LORA_ALPHA,
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "v_proj"],
        bias="none",
        lora_dropout=0.05
    )
    base_model = get_peft_model(base_model, peft_config)

    # C. Wrap TRL ValueHead
    model = AutoModelForCausalLMWithValueHead(base_model)
    model.v_head.requires_grad_(True)

    # =================================================================
    # üõ°Ô∏è MONKEY PATCHING BLOCK (V√Å L·ªñI QUAN TR·ªåNG)
    # =================================================================
    print("üõ°Ô∏è Applying Defensive Patches...")

    # 1. V√° l·ªói "base_model_prefix" (L·ªói b·∫°n v·ª´a g·∫∑p)
    # TRL c·∫ßn thu·ªôc t√≠nh n√†y ƒë·ªÉ t√¨m backbone, ta g√°n c·ª©ng l√† "model" (chu·∫©n Qwen/Llama)
    if not hasattr(model, "base_model_prefix"):
        model.base_model_prefix = "model"

    # 2. V√° l·ªói "object has no attribute 'model'"
    # M·ªôt s·ªë version TRL t√¨m self.model thay v√¨ self.pretrained_model
    if not hasattr(model, "model"):
        model.model = model.pretrained_model

    # 3. V√° l·ªói "generation_config"
    if not hasattr(model, "generation_config"):
        if hasattr(model.pretrained_model, "generation_config"):
            model.generation_config = model.pretrained_model.generation_config
        else:
            model.generation_config = GenerationConfig()

    # 4. ƒê√°nh d·∫•u model PEFT
    model.is_peft_model = True
    print("‚úÖ Patches applied.")
    # =================================================================

    tokenizer = AutoTokenizer.from_pretrained(Config.BASE_MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # =================================================================
    # 3. LOAD REWARD MODEL -> GPU 1
    # =================================================================
    print(f"Loading Reward Model on cuda:{device_reward}...")

    rm_base = AutoModelForSequenceClassification.from_pretrained(
        Config.BASE_MODEL_NAME,
        num_labels=2,
        quantization_config=bnb_config,
        device_map={"": device_reward},
        trust_remote_code=True
    )

    try:
        rm_model = PeftModel.from_pretrained(rm_base, Config.REWARD_ADAPTER_PATH)
    except Exception as e:
        print(f"‚ùå Error loading Reward Adapter: {e}")
        return

    # Pipeline Inference
    reward_pipe = pipeline(
        "text-classification",
        model=rm_model,
        tokenizer=tokenizer,
        batch_size=Config.BATCH_SIZE * 2,
        top_k=None
    )

    # =================================================================
    # 4. DATASET & TRAINER
    # =================================================================
    if not os.path.exists(Config.DATA_PPO_FILE):
        print("‚ùå Data PPO not found.")
        return

    dataset = load_dataset("json", data_files=Config.DATA_PPO_FILE, split="train")

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["query"])
        return sample

    dataset = dataset.map(tokenize, batched=False)

    def collator(data):
        return dict((key, [d[key] for d in data]) for key in data[0])

    config = PPOConfig(
        learning_rate=Config.LEARNING_RATE,
        batch_size=Config.BATCH_SIZE,
        mini_batch_size=1,
        gradient_accumulation_steps=Config.GRAD_ACCUM_STEPS,
    )

    # --- KH·ªûI T·∫†O TRAINER (CHU·∫®N STABLE TRL 0.12.0) ---
    ppo_trainer = PPOTrainer(
        config=config,  # D√πng 'config', KH√îNG d√πng 'args'
        model=model,
        ref_model=None,  # PEFT t·ª± lo ref_model
        tokenizer=tokenizer,  # D√πng 'tokenizer', KH√îNG d√πng 'processing_class'
        dataset=dataset,  # D√πng 'dataset', KH√îNG d√πng 'train_dataset'
        data_collator=collator
    )

    # =================================================================
    # 5. TRAINING LOOP
    # =================================================================
    print(">>> Training PPO...")

    generation_kwargs = {
        "min_length": -1,
        "top_k": 0.0,
        "top_p": 0.9,
        "do_sample": True,
        "pad_token_id": tokenizer.eos_token_id,
        "max_new_tokens": 128
    }

    step_count = 0
    for epoch, batch in enumerate(ppo_trainer.dataloader):
        query_tensors = batch["input_ids"]

        # A. Generate (GPU 0)
        response_tensors = []
        for query in query_tensors:
            q_tensor = torch.tensor(query).to(f"cuda:{device_policy}").unsqueeze(0)
            r = ppo_trainer.generate(q_tensor, **generation_kwargs)
            response_tensors.append(r.squeeze()[len(query):])

        batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

        # B. Reward (GPU 1)
        texts = [f"<|im_start|>user\n{q}<|im_end|>\n<|im_start|>assistant\n{r}<|im_end|>" for q, r in
                 zip(batch["query"], batch["response"])]
        pipe_outputs = reward_pipe(texts)

        rewards = []
        for output in pipe_outputs:
            score_good = 0.0
            for item in output:
                # L·∫•y score c·ªßa nh√£n '1' (T·ªët)
                if str(item['label']) in ['1', 'LABEL_1']:
                    score_good = item['score']
                    break

            # T√≠nh reward: (0.9 -> 0.4), (0.1 -> -0.4)
            final_reward = score_good - 0.5
            rewards.append(torch.tensor(final_reward).to(f"cuda:{device_policy}"))

        # C. Step (GPU 0)
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        step_count += 1
        print(f"Step {step_count}: Reward Avg = {torch.mean(torch.stack(rewards)).item():.4f}")

    # =================================================================
    # 6. SAVE
    # =================================================================
    print(">>> Saving PPO Adapter...")
    if not os.path.exists(Config.PPO_ADAPTER_PATH):
        os.makedirs(Config.PPO_ADAPTER_PATH)

    # Save adapter c·ªßa model g·ªëc
    model.pretrained_model.save_pretrained(Config.PPO_ADAPTER_PATH)
    print(f"‚úÖ ƒê√£ l∆∞u Adapter t·∫°i: {Config.PPO_ADAPTER_PATH}")


if __name__ == "__main__":
    train_ppo()